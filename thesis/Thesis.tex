\documentclass[12pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage{newlfont,wrapfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{xcolor}
\usepackage{subfiles}
\usepackage{listings}
\usepackage{tikz}
\usepackage{xspace}
\usepackage{float}
\usepackage{indentfirst}
\usepackage{url}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}


\input{solidity-highlighting.tex}	%Solidity highlighting

\textwidth=450pt\oddsidemargin=0pt
\newenvironment{dedication}
{%\clearpage           % we want a new page          %% I commented this
\thispagestyle{empty}% no header and footer
\vspace*{\stretch{1}}% some space at the top
\itshape             % the text is in italics
\raggedleft          % flush to the right margin
}
{\par % end the paragraph
\vspace{\stretch{3}} % space at bottom is three times that at the top
\clearpage           % finish off the page
}

\lstset{% This applies to ALL lstlisting
  basicstyle=\small\ttfamily\color{black},%
  breaklines=true,%
  numbers=none,
  moredelim=[s][\color{green!50!black}\ttfamily]{'}{'},% single quotes in green
}%


\begin{document}
\begin{titlepage}
    \begin{center}
    {{\Large{\textsc{Alma Mater Studiorum $\cdot$ Universit\`a di
    Bologna}}}} \rule[0.1cm]{15.8cm}{0.1mm}
    \rule[0.5cm]{15.8cm}{0.6mm}
    {\small{\bf SCUOLA DI SCIENZE\\
    Corso di Laurea Magistrale in Informatica }}
    \end{center}
    \vspace{15mm}
    \begin{center}
    {\LARGE{\bf Machine Learning}}\\
    \vspace{3mm}
    {\LARGE{\bf Vulneralibities Detection in SmartContracts }}\\
    \vspace{3mm}
    {\LARGE{\bf ..... }}\\
    \end{center}
    \vspace{40mm}
    \par
    \noindent
    \begin{minipage}[t]{0.47\textwidth}
    {\large{\bf Relatore:\\
    Chiar.mo Prof.\\
    Stefano Ferretti\\
    \\
    Correlatore:\\
    Dott. Stefano Pio Zingaro\\
    Dott. Saverio Giallorenzo\\
    }}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.47\textwidth}\raggedleft
    {\large{\bf Presentata da:\\
    Marco Benito Tomasone}}
    \end{minipage}
    \vspace{20mm}
    \begin{center}
    {\large{\bf Sessione I\\%inserire il numero della sessione in cui ci si laurea
    Anno Accademico 2023/2024}}%inserire l'anno accademico a cui si è iscritti
    \end{center}
    \end{titlepage}

    \begin{dedication}
        Alle nonne
    \end{dedication}
    \tableofcontents
    \listoffigures
\chapter{Introduzione}
Nel mondo delle tecnologie blockchain, gli smart contracts hanno guadagnato popolarità grazie alla loro abilità di automatizzare e far rispettare gli accordi tra due soggetti senza la necessità di un intermediario. 
Negli ultimi anni una delle tecnologie che sono spopolate e sono arrivate sulla bocca di tutti sono sicuramente la Blockchain e gli SmartContracts. Questi ultimi sono dei contratti digitali che permettono di eseguire delle operazioni in modo automatico e trasparente. Questi contratti sono scritti in un linguaggio di programmazione e vengono eseguiti su una macchina virtuale. Una delle principali caratteristiche peculiari degli SmartContracts è sicuramente la loro immutabilità, difatti una volta essere stati deployati sulla blockchain questi non possono più essere modificati.  Uno dei problemi principali degli SmartContracts è la sicurezza. Infatti, essendo dei contratti che vengono eseguiti in modo automatico, è possibile che ci siano delle vulnerabilità che possono essere sfruttate da malintenzionati. In questo lavoro di tesi verrà presentato un metodo per rilevare le vulnerabilità presenti negli SmartContracts. 
\subfile{Chapters/RelatedWork/RelatedWork.tex}

\subfile{Chapters/Dataset/Dataset.tex}
\chapter{Metodologia}
\section{BERT}
Il modello BERT (Bidirectional Encoder Representations from Transformers) rappresenta un pilastro fondamentale nel campo del Natural Language Processing (NLP), grazie alla sua capacità di comprensione del contesto delle parole all'interno di una frase o di un testo più ampio. Questo modello, sviluppato da Google, si basa sull'architettura dei transformer, una classe di modelli neurali che ha dimostrato notevole successo nell'analisi del linguaggio naturale.

BERT si distingue per la sua capacità bidirezionale di elaborare il contesto linguistico. A differenza dei modelli NLP precedenti, che processavano il testo in modo sequenziale, interpretando le parole una dopo l'altra, BERT considera sia il contesto precedente sia quello successivo di ciascuna parola all'interno di una frase. Questo approccio bidirezionale consente a BERT di catturare relazioni semantiche più complesse e di fornire una rappresentazione più accurata del significato del testo.

Il funzionamento di BERT può essere compreso attraverso due fasi principali: l'addestramento e l'utilizzo.

Durante la fase di addestramento, BERT viene esposto a enormi quantità di testo, proveniente da varie fonti e domini. Utilizzando un processo noto come "pre-addestramento", il modello apprende i modelli linguistici e il contesto delle parole. Questo pre-addestramento coinvolge due compiti principali: la predizione di parole mascherate e la predizione della successione di frasi. Nel primo compito, BERT impara a prevedere le parole mancanti in una frase fornita, mentre nel secondo compito, il modello apprende a determinare se due frasi sono consecutive in un testo o sono state estratte casualmente da testi diversi.

Una volta completata la fase di addestramento, BERT può essere utilizzato per una vasta gamma di compiti NLP senza la necessità di ulteriori addestramenti specifici. Durante l'utilizzo, il modello riceve in input una sequenza di token, che possono essere parole, frammenti di testo o segmenti di frasi. Ogni token viene rappresentato come un vettore di caratteristiche, derivato dal contesto bidirezionale fornito da BERT durante l'addestramento. Queste rappresentazioni vettoriali possono essere utilizzate per svariati compiti, come classificazione di testo, analisi del sentiment, risposta alle domande, traduzione automatica e molto altro ancora.

In sintesi, il modello BERT ha rivoluzionato il campo del NLP introducendo una comprensione più approfondita e contestualizzata del linguaggio naturale. La sua capacità di catturare il contesto bidirezionale delle parole ha portato a miglioramenti significativi nelle prestazioni dei sistemi NLP su una vasta gamma di compiti e applicazioni. BERT rimane un pilastro fondamentale nell'ambito della comprensione automatica del linguaggio umano, consentendo a macchine e sistemi di interagire e comprendere il linguaggio umano in modo più naturale e preciso.
\bibliographystyle{plain}
\bibliography{Bibliography.bib}
\nocite{*}
\end{document}
